# -*- coding: utf-8 -*-
"""to_be_uploded_ecr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IuqbvRmRq_LEVu2THqaX7N5kptt9MXLl

**Detailed Analysis and Comparison of various models on VW-E-golf**
"""

'''Import basic modules.'''
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf
import datetime

from scipy import stats
from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score,accuracy_score
from sklearn.model_selection import train_test_split, KFold


from sklearn.model_selection import RandomizedSearchCV,cross_val_score,GridSearchCV
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras import metrics

from sklearn.neural_network import MLPRegressor
import keras
from keras.layers import Dense
from scikeras.wrappers import KerasRegressor

from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.kernel_ridge import KernelRidge
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, VotingRegressor, BaggingRegressor,StackingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error



import tensorflow as tf
# import tensorflowjs
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from keras import regularizers
import tensorboard

'''Ignore deprecation and future, and user warnings.'''
import warnings as wrn
wrn.filterwarnings('ignore', category = DeprecationWarning)
wrn.filterwarnings('ignore', category = FutureWarning)
wrn.filterwarnings('ignore', category = UserWarning)

'''Set a seed for reproducibility'''
seed = 23

'''Initialize all the regression models object we are interested in.'''
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet,LogisticRegression
from sklearn.kernel_ridge import KernelRidge
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import mean_squared_error,log_loss

"""**Connector for S3 bucket**"""

# AWS_S3_BUCKET = 'aws-glue-ev-data'
# AWS_ACCESS_KEY_ID = 'AKIA2F3CXFNZQN3GZC6N'
# AWS_SECRET_ACCESS_KEY = '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'

# # data = '/content/drive/MyDrive/mitsubishi_imiev.csv'
# # df = pd.read_csv(data,encoding="ISO-8859-1")
# df = pd.read_csv(
#     "s3://aws-glue-ev-data/volkswagen_e_golf/volkswagen_e_golf_cleaned.csv",
#     storage_options={
#         "key": AWS_ACCESS_KEY_ID,
#         "secret": AWS_SECRET_ACCESS_KEY,
#     },
# )

"""**Local file paths**"""

#volkswagen_e_golf
#tesla_s
#renault_zoe
#mitsubishi_imiev
car_name = "ev_golf"
test = "/home/ubuntu/EV/Cleaned Data.csv"
in_path = f"C:/Users/shubh/OneDrive/Desktop/ev-app - Copy/cars/{car_name}/{car_name}.csv"
out_path_parent = f"C:/Users/shubh/OneDrive/Desktop/ev-app - Copy/cars/{car_name}/"

df = pd.read_csv(in_path)
col = ['consumption(kWh/100km)','manufacturer','model','version','fuel_date','fuel_type','power(kW)']
for i in col:
    if i in df.columns:
        df = df.drop(i,axis=1)
df['encoded_driving_style']  = df['encoded_driving_style'].astype('int')
df['park_heating']  = df['park_heating'].astype('int')

df['ecr_dev_type'] = df['ecr_deviation'].apply(lambda x: 1 if x >= 0 else 0 )
df.drop('ecr_deviation',axis=1,inplace=True)

df.shape

"""**Train Test Split (80-20)**"""

train,test  = train_test_split(df, test_size = 0.2, random_state = seed)

y_train = train['ecr_dev_type']
X_train = train.drop('ecr_dev_type',axis=1)

y_test = test['ecr_dev_type']
X_test = test.drop('ecr_dev_type',axis=1)

y_train = y_train.values
y_test = y_test.values

"""**Scaling data**"""

from sklearn.preprocessing import MinMaxScaler
scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()
X_trn_scl = scaler_X.fit_transform(X_train)
y_trn_scl = scaler_y.fit_transform(y_train.reshape(-1,1))
y_tst_scl = scaler_y.transform(y_test.reshape(-1,1))
X_tst_scl = scaler_X.transform(X_test)

"""**Train and Evaluate only on Training data**"""

'''Initialize models'''
from sklearn.svm import SVC

lr = LogisticRegression(n_jobs = -1)
svm = SVC(random_state = seed)
rf =  RandomForestClassifier(n_jobs = -1, random_state = seed)
ab = AdaBoostClassifier(random_state = seed)

models = [lr,svm, rf,  ab ]

training_score = []
for model in models:
    model.fit(X_train,y_train)
    y_pred_train = model.predict(X_train)
    train_mse = accuracy_score(y_train, y_pred_train)
    training_score.append(train_mse)
'''Plot the scores'''
train_score = pd.DataFrame(data = training_score, columns = ['Training_Accuracy'])
train_score.index = ['Logistic Reg',  'SVM',  'Random Forest', 'Ada Boost']
new_df  = train_score
train_score = train_score.sort_values(by = 'Training_Accuracy')

plt.figure(figsize=(15, 5))
sns.set(style="darkgrid")
sns.barplot(x=train_score.index, y='Training_Accuracy', data=train_score,palette="rocket")

plt.xlabel('Models')
plt.ylabel('Training_Accuracy')
plt.title('Training_Accuracy vs Models')
#plt.show()

"""**Train on Training data and Evaluate on Validation data**"""

'''Evaluate models on the holdout set(say on 20%).'''
def train_test_split_score(model):

    X_train_, X_test_val, Y_train_, Y_test_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = seed)
    model.fit(X_train_, Y_train_)
    prediction = model.predict(X_test_val)
    mae = accuracy_score(prediction, Y_test_val)
    return mae

'''Calculate train_validate_split score of differnt models and plot them.'''
models =  [lr, svm, rf, ab]
train_test_split_rmse = []
for model in models:
    train_test_split_rmse.append(train_test_split_score(model))

train_score = pd.DataFrame(data = train_test_split_rmse, columns = ['Train Validation Accuracy'])
train_score.index = ['Logistic Reg',  'SVM',  'Random Forest', 'Ada Boost']
new_df['Train Validation Accuracy'] = train_score
train_score = train_score.sort_values(by = 'Train Validation Accuracy')

print(train_score)

plt.figure(figsize=(15, 5))
sns.set(style="darkgrid")
sns.barplot(x=train_score.index, y='Train Validation Accuracy', data=train_score,palette="rocket")
plt.xlabel('Models')
plt.ylabel('Train Validation Accuracy')
plt.title('Train Validation Accuracy vs models')
#plt.show()

"""**Comapre scores**"""

new_df = new_df.sort_values(by='Train Validation Accuracy')
sns.set(style="whitegrid")
plt.figure(figsize=(15, 5))
df_reset = new_df.reset_index()
df_melted = df_reset.melt(id_vars='index', var_name='Metric', value_name='Value')
sns.barplot(x='index', y='Value', hue='Metric', data=df_melted, palette="pastel")

plt.xlabel("Models")
plt.ylabel("Accuracy")
plt.title("Accuracy vs Models")
plt.legend(title="Metrics")
#plt.show()

"""**Train and Test using k-Fold cross validation**"""

'''Function to compute cross validation scores.'''
num_folds = 5

# Create a k-fold cross-validation object
kf = KFold(n_splits=num_folds, shuffle=True,random_state=seed)

def cross_validate(model):
    neg_x_val_score = cross_val_score(model, X_train, y_train, cv = kf, n_jobs = -1, scoring = 'accuracy')
    x_val_score = np.round(neg_x_val_score, 5)
    return x_val_score.mean()

'''Calculate cross validation score of differnt models and plot them.'''
models = [lr, svm,rf, ab]
cross_val_scores = []
for model in models:
    cross_val_scores.append(cross_validate(model))


x_val_score = pd.DataFrame(data = cross_val_scores, columns = ['Cross Validation Scores (Accuracy)'])
x_val_score.index = ['Logistic Reg',  'SVM',  'Random Forest', 'Ada Boost']
x_val_score = x_val_score.round(5)
x = x_val_score.index
y = x_val_score['Cross Validation Scores (Accuracy)']
title = "Models' 10-fold Cross Validation Scores (MAE)"
x_val_score  = x_val_score.sort_values(by = 'Cross Validation Scores (Accuracy)')
print(x_val_score)


plt.figure(figsize=(15, 5))
sns.set(style="darkgrid")
sns.barplot(x=x_val_score.index, y='Cross Validation Scores (Accuracy)', data=x_val_score,palette="rocket")
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Cross Validation Scores (Accuracy) vs models')
#plt.show()

"""**Comapre scores**"""

new_df = new_df.sort_index()
x_val_score = x_val_score.sort_index()
new_df['Cross Validation Scores (Accuracy)'] = x_val_score['Cross Validation Scores (Accuracy)']
new_df = new_df.sort_values(by='Cross Validation Scores (Accuracy)')
sns.set(style="whitegrid")


plt.figure(figsize=(15, 5))
df_reset = new_df.reset_index()
df_melted = df_reset.melt(id_vars='index', var_name='Metric', value_name='Value')
print(df_reset)
sns.barplot(x='index', y='Value', hue='Metric', data=df_melted, palette="pastel")

plt.xlabel("Model")
plt.ylabel("Accuracy")
plt.title("Accuracy vs Models")
plt.legend(title="Metrics")
#plt.show()

"""**Fine Tune Models**"""

def grid_search_cv(model, params):

    kfold = KFold(n_splits=5,shuffle=True, random_state=seed)
    global best_params, best_score

    # grid_search = GridSearchCV(estimator = model, param_grid = params, cv = kfold, verbose = 1,
    #                         scoring = 'neg_mean_absolute_error', n_jobs = -1)
    grid_search = RandomizedSearchCV(estimator = model, param_distributions=params,
                          verbose=1,n_iter=5, random_state=seed,
                   cv=kfold, scoring='accuracy',n_jobs = -1)

    grid_search.fit(X_train, y_train)
    best_params = grid_search.best_params_
    best_score = (np.round(grid_search.best_score_, 5))
    return best_params, best_score

'''Define hyperparameters of logistic .'''
lr_params = {'penalty':['l1','l2','elasticnet',None],
          'multi_class':['ovr', 'multinomial'],
          'C': np.linspace(0.001, 5, 2),
          'l1_ratio':np.linspace(0,1,2)}
grid_search_cv(lr, lr_params)
lr_best_params, lr_best_score = best_params, best_score
print('lr  best params:{} & best_score:{:0.5f}' .format(lr_best_params, lr_best_score))

'''Define hyperparameters of support vector machine'''
svm_params = {
    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], # precomputed is omitted from kernel to avoid error.
    'C': np.linspace(0.001, 1, 2),
    'gamma':np.linspace(0.001, 1, 2)}

grid_search_cv(svm, svm_params)
svm_best_params, svm_best_score = best_params, best_score
print('SVM best params:{} & best_score:{:0.5f}' .format(svm_best_params, svm_best_score))
# Don't bother it takes some time. Training is usually more slower in svm.

'''Define hyperparameters of rf tree'''

rf_params = {'max_features':[2,3],'min_samples_leaf':[2]}

grid_search_cv(rf, rf_params)
rf_best_params, rf_best_score = best_params, best_score
print('RF best params:{} & best_score:{:0.5f}' .format(rf_best_params ,rf_best_score))

ab_params = {'learning_rate':np.linspace(0.001,0.8, 2),
          'n_estimators':[50]}
grid_search_cv(ab, ab_params)
ab_best_params, ab_best_score = best_params, best_score
print('AB best params:{} & best_score:{:0.5f}' .format(ab_best_params ,ab_best_score))

"""Let's plot the models' rmse after optimization."""
optimized_scores = pd.DataFrame({'Optimized Scores':[lr_best_score, svm_best_score, rf_best_score,ab_best_score] })
optimized_scores.index = ['Logistic Reg',  'SVM',  'Random Forest', 'Ada Boost']
comp_scores = optimized_scores
optimized_scores = optimized_scores.sort_values(by = 'Optimized Scores')
optimized_scores
plt.figure(figsize=(15, 5))
sns.set(style="darkgrid")
sns.barplot(x=optimized_scores.index, y='Optimized Scores', data=optimized_scores,palette="rocket")
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Optimized_Accuracy vs models')
#plt.show()

"""**Final Train and Test on Unseen data using Optimized models**"""

'''Initialize 3 object models with best hyperparameters'''
lr_opt = LogisticRegression(**lr_best_params)
svm_opt = SVC(**svm_best_params)
rf_opt =  RandomForestClassifier(**rf_best_params)
ab_opt = AdaBoostClassifier(**ab_best_params)

scores = {'accuracy_score_train':[],'accuracy_score_test':[],'log_loss_train':[],'log_loss_test':[]}

def report_results(training_pred, test_pred):
    scores['accuracy_score_train'].append((accuracy_score(y_true=y_train, y_pred=training_pred)))
    scores['accuracy_score_test'].append(accuracy_score(y_true=y_test, y_pred=test_pred))
    scores['log_loss_train'].append(log_loss(y_true=y_train, y_pred=training_pred))
    scores['log_loss_test'].append(log_loss(y_true=y_test, y_pred=test_pred))

'''Now train and predict with optimized models'''
def predict_with_optimized_models(model):
    model.fit(X_train, y_train)
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    mse = accuracy_score(y_pred_test, y_test)
    report_results(y_pred_train,y_pred_test)
    return mse

models = [lr_opt, svm_opt,rf_opt,ab_opt]

final_test_scores = []
for model in models:
    final_test_scores.append(predict_with_optimized_models(model))

final_test_scores_df = pd.DataFrame({'final_test_scores':np.round(final_test_scores, 5)})
final_test_scores_df.index = ['Logistic Reg',  'SVM',  'Random Forest', 'Ada Boost']
final_test_scores_df= final_test_scores_df.sort_values(by = 'final_test_scores')
final_test_scores_df

plt.figure(figsize=(15, 5))
sns.set(style="darkgrid")
sns.barplot(x=final_test_scores_df.index, y='final_test_scores', data=final_test_scores_df,palette="rocket")
plt.xlabel('Models')
plt.ylabel('MAE')
plt.title('Test_MAE vs models')
#plt.show()

"""**Compare scores**"""

# new_df = new_df.sort_values(by='index')
comp_scores = comp_scores.sort_index()
final_test_scores_df = final_test_scores_df.sort_index()
comp_scores['final_test_scores'] = final_test_scores_df['final_test_scores']

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Create a sample DataFrame
comp_scores = comp_scores.sort_values(by='final_test_scores')
# Set the style of the plot
sns.set(style="darkgrid")

# Set the dimensions of the plot
plt.figure(figsize=(15, 5))

# Melt the DataFrame for easier plotting
df_reset = comp_scores.reset_index()
print(df_reset)
df_melted = df_reset.melt(id_vars='index', var_name='Metric', value_name='Value')

# Create a grouped bar chart using Seaborn
sns.barplot(x='index', y='Value', hue='Metric', data=df_melted, palette="pastel")

plt.xlabel("Model")
plt.ylabel("MAE")
plt.title("MAE vs Models")

# Add a legend
plt.legend(title="Metrics")

#plt.show()

scores

all_models =['Logistic Reg',  'SVM',  'Random Forest', 'Ada Boost']
overall_scores = pd.DataFrame(scores,index=all_models)
overall_scores

"""**Ensemble methods**"""

import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import matplotlib.pyplot as plt
from sklearn.metrics import log_loss
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.ensemble import VotingClassifier,BaggingClassifier,StackingClassifier

voting = VotingClassifier([('LR',lr), ('RF',rf_opt), ('AB',ab_opt), ('SVM',svm_opt)])
voting.fit(X_train, y_train)
y_pred_train = voting.predict(X_train)
y_pred_test = voting.predict(X_test)
report_results(y_pred_train,y_pred_test)
all_models.append('Voting Clf')
print(accuracy_score(y_test, y_pred_test))

overall_scores = pd.DataFrame(scores,index=all_models)

bagging = BaggingClassifier(random_state=23,
                           n_jobs=-1)

params = {'estimator': [lr,rf_opt,ab_opt,svm_opt],
          'n_estimators':[5]}
gcv = GridSearchCV(bagging, param_grid=params,n_jobs=-1,
                   cv=2, scoring='accuracy')
gcv.fit(X_train, y_train)
print(gcv.best_params_)
print(gcv.best_score_)
bagging = BaggingClassifier(**gcv.best_params_)
bagging.fit(X_train, y_train)


y_pred_train = bagging.predict(X_train)
y_pred_test = bagging.predict(X_test)
report_results(y_pred_train,y_pred_test)
all_models.append('Bagging Clf')
overall_scores = pd.DataFrame(scores,index=all_models)

"""
**MLP Regressor**"""

import warnings
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier

from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import keras
from keras.layers import Dense
from scikeras.wrappers import KerasClassifier


# scale the values
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
# y_train = sc.fit_transform(X=y_train)
# y_test = sc.fit_transform(X=y_test)

"""define the shallow multi-layer perceptron model"""
mlp = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000, n_iter_no_change=100, activation='relu',
                   solver='adam', verbose=False, warm_start=False)
print(cross_validate(mlp))
print(predict_with_optimized_models(mlp))

all_models.append('MLP')
print(scores)
overall_scores = pd.DataFrame(scores,index=all_models)
overall_scores

"""**Deep Neural Network**"""

tf.random.set_seed(2023)
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(15, activation='relu',input_shape=(X_trn_scl.shape[1],)),
    tf.keras.layers.Dense(15, activation='relu'),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(5, activation='relu'),
    tf.keras.layers.Dense(3, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])


batch_size = 16
epochs = 1000
STEPS_PER_EPOCH = X_trn_scl.shape[0]/batch_size
es = keras.callbacks.EarlyStopping(monitor='loss', patience=50, verbose=0)

lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
  0.001,
  decay_steps = STEPS_PER_EPOCH * 100,
  decay_rate=0.98,
  staircase=False
)

opt = keras.optimizers.Adam(lr_schedule)

model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
model.summary()

history  = model.fit(X_trn_scl,y_trn_scl, epochs=epochs, batch_size=batch_size, callbacks=[es], verbose=1)

predictions = (model.predict(X_tst_scl))

predictions = tf.where(predictions > 0.7, tf.ones_like(predictions), tf.zeros_like(predictions) )
accuracy_score(y_test, predictions)

y_pred_train_ = model.predict(X_trn_scl)
y_pred_train = tf.where(y_pred_train_ > 0.7, tf.ones_like(y_pred_train_), tf.zeros_like(y_pred_train_) )
y_pred_test_ = model.predict(X_tst_scl)
y_pred_test = tf.where(y_pred_test_ > 0.7, tf.ones_like(y_pred_test_), tf.zeros_like(y_pred_test_) )
report_results(y_pred_train,y_pred_test)

"""**Model Comparisons**"""

all_models.append('DeepMLP')
print(scores)
overall_scores = pd.DataFrame(scores,index=all_models)
overall_scores

# new_df = new_df.sort_values(by='index')
comp_scores = overall_scores[['accuracy_score_train','accuracy_score_test']]

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Create a sample DataFrame
comp_scores = comp_scores.sort_values(by='accuracy_score_test')
# Set the style of the plot
sns.set(style="darkgrid")

# Set the dimensions of the plot
plt.figure(figsize=(15, 5))

# Melt the DataFrame for easier plotting
df_reset = comp_scores.reset_index()
print(df_reset)
df_melted = df_reset.melt(id_vars='index', var_name='Metric', value_name='Value')

# Create a grouped bar chart using Seaborn
sns.barplot(x='index', y='Value', hue='Metric', data=df_melted, palette="pastel")

plt.xlabel("Model")
plt.ylabel("MAE")
plt.title("MAE vs Models")

# Add a legend
plt.legend(title="Metrics")

#plt.show()

# """**Learning Curves**"""

# from yellowbrick.datasets import load_energy
# from yellowbrick.model_selection import LearningCurve

# # Load a regression dataset
# y = df['ecr_dev_type']
# X = df.drop('ecr_dev_type',axis=1)


# models = [lr, rf_opt,ab_opt,bagging,mlp]
# #Instantiate the regression model and visualizer
# model = models[0]
# for i in models:

#     visualizer = LearningCurve(i, scoring='neg_log_loss')

#     visualizer.fit(X, y)        # Fit the data to the visualizer
#     visualizer.show()           # Finalize and render the figure

# """**Feature Importances**"""

# from sklearn.ensemble import RandomForestClassifier

# from yellowbrick.datasets import load_occupancy
# from yellowbrick.model_selection import FeatureImportances

# # Load the classification data set

# models = [lr, svm_opt, rf_opt,ab_opt]

# for model in models:
#     viz = FeatureImportances(model)
#     viz.fit(X, y)
#     viz.show()

# overall_scores

# all_models

# models

models = [lr_opt, svm_opt,rf_opt,ab_opt,voting,bagging,mlp,model]

best_score = overall_scores['accuracy_score_test'].max()
best_model_name = overall_scores['accuracy_score_test'].idxmax()
best_model = models[all_models.index(best_model_name)]
print(best_score)
print(best_model_name)
print(best_model)

str(best_model)

import csv

model_details = {
    'mae': best_score,
    'params': str(best_model),
    'name': best_model_name
}

csv_file = out_path_parent+'model_details_c.csv'

# Open the CSV file in write mode
with open(csv_file, 'w', newline='') as csvfile:
    field_names = model_details.keys()

    # Create a CSV writer and write the header
    writer = csv.DictWriter(csvfile, fieldnames=field_names)
    writer.writeheader()

    # Write the model details as a row
    writer.writerow(model_details)

print(f"Model details saved as {csv_file}")

import pickle

if best_model_name == 'DeepMLP':
    best_model.save(out_path_parent+"model_c.keras")
else:
    with open(out_path_parent+'model_c.pkl', 'wb') as files:
        pickle.dump(best_model, files)


with open(out_path_parent+'scaler_X_c.pkl', 'wb') as files:
        pickle.dump(scaler_X, files)

