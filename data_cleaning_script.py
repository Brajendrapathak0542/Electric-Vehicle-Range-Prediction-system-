# -*- coding: utf-8 -*-
"""Data_Cleaning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q6ifgMay0QZqfpHDwvf_IH8UzrZeEy38
"""

# import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sb
import os
from IPython.display import display
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OrdinalEncoder
from sklearn.impute import SimpleImputer

from google.colab import drive
drive.mount('/content/drive')

data = '/content/drive/MyDrive/volkswagen_e_golf.csv'
df = pd.read_csv(data,encoding="ISO-8859-1")

# Reading data and removing unnecessary column
def remove_feature(data):
  new_df = data.drop(['manufacturer','model','version','fuel_date','fuel_type'],axis=1)
  if 'fuel_note' in new_df.columns:
    # Remove the specified column
    new_df.drop('fuel_note',axis=1, inplace=True)
  return new_df

ev_data = remove_feature(df)

ev_data.dtypes

# Changing datatypes of columns
ev_data['city'] = ev_data['city'].astype('object')
ev_data['motor_way'] = ev_data['motor_way'].astype('object')
ev_data['country_roads'] = ev_data['country_roads'].astype('object')
ev_data['A/C'] = ev_data['A/C'].astype('object')
ev_data['park_heating'] = ev_data['park_heating']
if ev_data['trip_distance(km)'].dtypes == 'object':
  ev_data['trip_distance(km)']= ev_data['trip_distance(km)'].str.split(",").str[0]
ev_data['trip_distance(km)']= ev_data['trip_distance(km)'].astype('float')

# Removing Trip Distance Rows with NULL values
ev_data = ev_data[ev_data['trip_distance(km)'].isnull() == False]

# Get columns with null values
null_columns = ev_data.columns[ev_data.isnull().any()]

# Create a dictionary to store column names and their corresponding null percentages
null_percentages = {}

# Calculate null percentages
for column in null_columns:
    null_percentage = (ev_data[column].isnull().sum() / len(ev_data)) * 100
    null_percentages[column] = null_percentage

# Convert the dictionary to a DataFrame
null_df = pd.DataFrame.from_dict(null_percentages, orient='index', columns=['Null Percentage'])

print("Columns with null values and their corresponding null percentages:")
print(null_df)

threshold = 30
columns_to_drop = null_df[null_df['Null Percentage'] > threshold].index

# Drop columns from the original DataFrame
df_cleaned = ev_data.drop(columns=columns_to_drop)
df_cleaned

df_cleaned.isnull().sum()

categorical_columns = [col for col in df_cleaned.columns if pd.api.types.is_categorical_dtype(df_cleaned[col]) or df_cleaned[col].dtype == 'object']

# Fill NaN values in categorical columns with the majority (mode) value
for column in categorical_columns:
    mode_value = df_cleaned[column].mode()[0]  # Get the mode value of the column
    df_cleaned[column].fillna(mode_value, inplace=True)

print("DataFrame after filling NaN values with majority (mode) values in categorical columns:")
df_cleaned

numerical_columns = df_cleaned.select_dtypes(include=['int64','float64']).columns

# Create the SimpleImputer with strategy='mean'
imputer = SimpleImputer(strategy='mean')

# Fit and transform the imputer on the numerical columns
df_cleaned[numerical_columns] = imputer.fit_transform(df_cleaned[numerical_columns])

print("DataFrame after filling NaN values with mean in numerical columns:")
df_cleaned

df_cleaned.isnull().sum()

outlier_columns=df_cleaned.select_dtypes(exclude='object')

def count_outliers(data,column):
    count=0
    q1=data[column].describe()[4]
    q3=data[column].describe()[6]
    iqr=q3-q1
    for i in data[column]:
        if (i<q1-(1.5*iqr)) or (i>q3+(1.5*iqr)):
            count+=1
    return count

for column in outlier_columns:
    print("No of outliers in {} are {}".format(column,count_outliers(df_cleaned,column)))

def remove_outliers(data,column_list):
    for column in column_list:
        q1=data[column].describe()[4]
        q3=data[column].describe()[6]
        iqr=q3-q1
        for i in data[column]:
            if (i<q1-(1.5*iqr)) or (i>q3+(1.5*iqr)):
                data = data.loc[data[column] != i]
    return data

data=remove_outliers(df_cleaned,outlier_columns)

data.shape

ordinalEncoder = OrdinalEncoder(categories=[['Normal', 'Moderate', 'Fast']])
data['encoded_driving_style'] = ordinalEncoder.fit_transform(data.driving_style.values.reshape(-1,1))
data.drop("driving_style",axis=1,inplace=True)

labelEncoder = LabelEncoder()
data['encoded_tire_type'] = labelEncoder.fit_transform(data.tire_type)
data.drop("tire_type",axis=1,inplace=True)

data.head()

data[['city','motor_way','country_roads','A/C']] = pd.get_dummies(data[['city','motor_way','country_roads','A/C']],drop_first=True)
data.columns

data

data.columns

data.dtypes

data.to_csv("Cleaned Data.csv",index=False)

data.head()

