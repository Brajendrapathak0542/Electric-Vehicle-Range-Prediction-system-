# -*- coding: utf-8 -*-
"""to_be_uploded_dr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KTg3cpAohWNMdeRH-4fb6tVdMV8Z14Hq

**Detailed Analysis and Comparison of various models on VW-E-golf**
"""

'''Import basic modules.'''
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf
import datetime

from scipy import stats
from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score
from sklearn.model_selection import train_test_split, KFold


from sklearn.model_selection import RandomizedSearchCV,cross_val_score,GridSearchCV
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras import metrics

from sklearn.neural_network import MLPRegressor
import keras
from keras.layers import Dense
from scikeras.wrappers import KerasRegressor

from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.kernel_ridge import KernelRidge
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, VotingRegressor, BaggingRegressor,StackingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error



import tensorflow as tf
# import tensorflowjs
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from keras import regularizers
import tensorboard

'''Ignore deprecation and future, and user warnings.'''
import warnings as wrn
wrn.filterwarnings('ignore', category = DeprecationWarning)
wrn.filterwarnings('ignore', category = FutureWarning)
wrn.filterwarnings('ignore', category = UserWarning)

'''Set a seed for reproducibility'''
seed = 23

"""**Connector for S3 bucket**"""

# AWS_S3_BUCKET = 'aws-glue-ev-data'
# AWS_ACCESS_KEY_ID = 'AKIA2F3CXFNZQN3GZC6N'
# AWS_SECRET_ACCESS_KEY = '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'

# # data = '/content/drive/MyDrive/mitsubishi_imiev.csv'
# # df = pd.read_csv(data,encoding="ISO-8859-1")
# df = pd.read_csv(
#     "s3://aws-glue-ev-data/volkswagen_e_golf/volkswagen_e_golf_cleaned.csv",
#     storage_options={
#         "key": AWS_ACCESS_KEY_ID,
#         "secret": AWS_SECRET_ACCESS_KEY,
#     },
# )

"""**Local file paths**"""

#ev_golf
#tesla_s
#renault_zoe
#mitsubishi_imiev
car_name = "ev_golf"
test = "/home/ubuntu/EV/Cleaned Data.csv"
in_path = f"C:/Users/shubh/OneDrive/Desktop/ev-app - Copy/cars/{car_name}/{car_name}.csv"
out_path_parent = f"C:/Users/shubh/OneDrive/Desktop/ev-app - Copy/cars/{car_name}/"

df = pd.read_csv(in_path)
col = ['ecr_deviation','consumption(kWh/100km)','manufacturer','model','version','fuel_date','fuel_type','power(kW)']
for i in col:
    if i in df.columns:
        df = df.drop(i,axis=1)
df['encoded_driving_style']  = df['encoded_driving_style'].astype('int')
df['park_heating']  = df['park_heating'].astype('int')

"""**Train Test Split (80-20)**"""

train,test  = train_test_split(df, test_size = 0.2, random_state = seed)

y_train = train['trip_distance(km)']
X_train = train.drop('trip_distance(km)',axis=1)

y_test = test['trip_distance(km)']
X_test = test.drop('trip_distance(km)',axis=1)

y_train = y_train.values
y_test = y_test.values

"""**Scaling data**"""

from sklearn.preprocessing import MinMaxScaler
scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()
X_trn_scl = scaler_X.fit_transform(X_train)
y_trn_scl = scaler_y.fit_transform(y_train.reshape(-1,1))
y_tst_scl = scaler_y.transform(y_test.reshape(-1,1))
X_tst_scl = scaler_X.transform(X_test)

"""**Train and Evaluate only on Training data**"""

'''Initialize models'''
linear = LinearRegression()
elnt = ElasticNet(random_state = seed)
rf =  RandomForestRegressor(random_state = seed)
ab = AdaBoostRegressor(random_state = seed)

models = [linear,elnt, rf,  ab ]
training_score = []

'''Evaluate models on the training set'''
for model in models:

    model.fit(X_train,y_train)
    y_pred_train = model.predict(X_train)
    train_mse = mean_absolute_error(y_train, y_pred_train)
    training_score.append(train_mse)

'''Plot the scores'''
train_score = pd.DataFrame(data = training_score, columns = ['Training MAE'])
train_score.index = ['Linear Reg',  'Elastic Net Reg',  'Random Forest Reg', 'AdaBoost Reg']
new_df  = train_score
train_score = train_score.sort_values(by = 'Training MAE')

plt.figure(figsize=(15, 5))
sns.set(style="darkgrid")
sns.barplot(x=train_score.index, y='Training MAE', data=train_score,palette="rocket")

plt.xlabel('Models')
plt.ylabel('Training MAE')
plt.title('Training Mean Absolute Error vs Models')
#plt.show()

"""**Train on Training data and Evaluate on Validation data**"""

'''Evaluate models on the holdout set(say on 20%).'''
def train_test_split_score(model):

    X_train_, X_test_val, Y_train_, Y_test_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = seed)
    model.fit(X_train_, Y_train_)
    prediction = model.predict(X_test_val)
    mae = mean_absolute_error(prediction, Y_test_val)
    return mae

'''Calculate train_validate_split score of differnt models and plot them.'''
models = [linear, elnt, rf, ab]
train_test_split_rmse = []
for model in models:
    train_test_split_rmse.append(train_test_split_score(model))

train_score = pd.DataFrame(data = train_test_split_rmse, columns = ['Train Validation MAE'])
train_score.index = ['Linear Reg',  'Elastic Net Reg',  'Random Forest Reg', 'AdaBoost Reg']
new_df['Train Validation MAE'] = train_score
train_score = train_score.sort_values(by = 'Train Validation MAE')

print(train_score)

plt.figure(figsize=(15, 5))
sns.set(style="darkgrid")
sns.barplot(x=train_score.index, y='Train Validation MAE', data=train_score,palette="rocket")
plt.xlabel('Models')
plt.ylabel('Train Validation MAE')
plt.title('Train Validation MAE vs models')
#plt.show()

"""**Comapre scores**"""

new_df = new_df.sort_values(by='Train Validation MAE')
sns.set(style="whitegrid")
plt.figure(figsize=(15, 5))
df_reset = new_df.reset_index()
df_melted = df_reset.melt(id_vars='index', var_name='Metric', value_name='Value')
sns.barplot(x='index', y='Value', hue='Metric', data=df_melted, palette="pastel")

plt.xlabel("Models")
plt.ylabel("MAE")
plt.title("MAE vs Models")
plt.legend(title="Metrics")
#plt.show()

"""**Train and Test using k-Fold cross validation**"""

'''Function to compute cross validation scores.'''
num_folds = 5

# Create a k-fold cross-validation object
kf = KFold(n_splits=num_folds, shuffle=True,random_state=seed)

def cross_validate(model):
    neg_x_val_score = cross_val_score(model, X_train, y_train, cv = kf, n_jobs = -1, scoring = 'neg_mean_absolute_error')
    x_val_score = np.round((-1*neg_x_val_score), 5)
    return x_val_score.mean()

'''Calculate cross validation score of differnt models and plot them.'''
models = [linear, elnt,rf, ab]
cross_val_scores = []
for model in models:
    cross_val_scores.append(cross_validate(model))


x_val_score = pd.DataFrame(data = cross_val_scores, columns = ['Cross Validation Scores (MAE)'])
x_val_score.index = ['Linear Reg',  'Elastic Net Reg',  'Random Forest Reg', 'AdaBoost Reg']
x_val_score = x_val_score.round(5)
x = x_val_score.index
y = x_val_score['Cross Validation Scores (MAE)']
title = "Models' 10-fold Cross Validation Scores (MAE)"
x_val_score  = x_val_score.sort_values(by = 'Cross Validation Scores (MAE)')
print(x_val_score)


plt.figure(figsize=(15, 5))
sns.set(style="darkgrid")
sns.barplot(x=x_val_score.index, y='Cross Validation Scores (MAE)', data=x_val_score,palette="rocket")
plt.xlabel('Models')
plt.ylabel('MAE')
plt.title('Cross Validation Scores (MAE) vs models')
#plt.show()

"""**Comapre scores**"""

new_df = new_df.sort_index()
x_val_score = x_val_score.sort_index()
new_df['Cross Validation Scores (MAE)'] = x_val_score['Cross Validation Scores (MAE)']
new_df = new_df.sort_values(by='Cross Validation Scores (MAE)')
sns.set(style="whitegrid")


plt.figure(figsize=(15, 5))
df_reset = new_df.reset_index()
df_melted = df_reset.melt(id_vars='index', var_name='Metric', value_name='Value')
print(df_reset)
sns.barplot(x='index', y='Value', hue='Metric', data=df_melted, palette="pastel")

plt.xlabel("Model")
plt.ylabel("MAE")
plt.title("MAE vs Models")
plt.legend(title="Metrics")
#plt.show()

"""**Fine Tune Models**"""

def grid_search_cv(model, params):

    kfold = KFold(n_splits=5,shuffle=True, random_state=seed)
    global best_params, best_score

    # grid_search = GridSearchCV(estimator = model, param_grid = params, cv = kfold, verbose = 1,
    #                         scoring = 'neg_mean_absolute_error', n_jobs = -1)
    grid_search = RandomizedSearchCV(estimator = model, param_distributions=params,
                          verbose=1,n_iter=5, random_state=seed,
                   cv=kfold, scoring='neg_mean_absolute_error',n_jobs = -1)

    grid_search.fit(X_train, y_train)
    best_params = grid_search.best_params_
    best_score = -1*(np.round(grid_search.best_score_, 5))
    return best_params, best_score

'''Define hyperparameters of Elastic net.'''
elastic_params = {'alpha': np.linspace(0.001, 15,20),
                    'l1_ratio': np.linspace(0.001, 0.999, 25),
                    'random_state':[seed]}
grid_search_cv(elnt, elastic_params)
elastic_best_params, elastic_best_score = best_params, best_score
print('Elastic Net best params:{} & best_score:{:0.5f}' .format(elastic_best_params, elastic_best_score))

'''Define hyperparameters of rf tree'''

rf_params = {'max_features':[2,3]}

grid_search_cv(rf, rf_params)
rf_best_params, rf_best_score = best_params, best_score
print('RF best params:{} & best_score:{:0.5f}' .format(rf_best_params ,rf_best_score))

ab_params = {'learning_rate':np.linspace(0.001,0.8, 2),
          'n_estimators':[50]}
grid_search_cv(ab, ab_params)
ab_best_params, ab_best_score = best_params, best_score
print('AB best params:{} & best_score:{:0.5f}' .format(ab_best_params ,ab_best_score))

"""Let's plot the models' mse after optimization."""
optimized_scores = pd.DataFrame({'Optimized Scores':np.round([x_val_score.loc['Linear Reg','Cross Validation Scores (MAE)'],
                  elastic_best_score, rf_best_score,ab_best_score], 5)})
optimized_scores.index = ['Linear Reg',  'Elastic Net Reg',  'Random Forest Reg', 'AdaBoost Reg']
comp_scores = optimized_scores
optimized_scores = optimized_scores.sort_values(by = 'Optimized Scores')
optimized_scores

plt.figure(figsize=(15, 5))
sns.set(style="darkgrid")
sns.barplot(x=optimized_scores.index, y='Optimized Scores', data=optimized_scores,palette="rocket")
plt.xlabel('Models')
plt.ylabel('MAE')
plt.title('Optimized_MAE vs models')
#plt.show()

"""**Final Train and Test on Unseen data using Optimized models**"""

'''Initialize 3 object models with best hyperparameters'''
elastic_net_opt = ElasticNet(**elastic_best_params)
rf_opt =  RandomForestRegressor(**rf_best_params)
ab_opt = AdaBoostRegressor(**ab_best_params)

scores = {'RMSE_train':[],'RMSE_test':[],'MAE_train':[],'MAE_test':[],'r2_train':[],'r2_test':[]}

def report_results(training_pred, test_pred):
    scores['RMSE_train'].append(round(np.sqrt(mean_squared_error(y_true=y_train, y_pred=training_pred)),3))
    scores['RMSE_test'].append(round(np.sqrt(mean_squared_error(y_true=y_test, y_pred=test_pred)),3))
    scores['MAE_train'].append(round(mean_absolute_error(y_true=y_train, y_pred=training_pred),3))
    scores['MAE_test'].append(round(mean_absolute_error(y_true=y_test, y_pred=test_pred),3))
    scores['r2_train'].append(round(r2_score(y_true=y_train, y_pred=training_pred),3))
    scores['r2_test'].append(round(r2_score(y_true=y_test, y_pred=test_pred),3))

'''Now train and predict with optimized models'''
def predict_with_optimized_models(model):
    model.fit(X_train, y_train)
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    mse = mean_absolute_error(y_pred_test, y_test)
    report_results(y_pred_train,y_pred_test)
    return mse

models = [linear, elastic_net_opt,rf_opt,ab_opt]

final_test_scores = []
for model in models:
    final_test_scores.append(predict_with_optimized_models(model))

final_test_scores_df = pd.DataFrame({'final_test_scores':np.round(final_test_scores, 5)})
final_test_scores_df.index = ['Linear Reg',  'Elastic Net Reg',  'Random Forest Reg', 'AdaBoost Reg']
final_test_scores_df= final_test_scores_df.sort_values(by = 'final_test_scores')
final_test_scores_df

plt.figure(figsize=(15, 5))
sns.set(style="darkgrid")
sns.barplot(x=final_test_scores_df.index, y='final_test_scores', data=final_test_scores_df,palette="rocket")
plt.xlabel('Models')
plt.ylabel('MAE')
plt.title('Test_MAE vs models')
#plt.show()

"""**Compare scores**"""

comp_scores = comp_scores.sort_index()
final_test_scores_df = final_test_scores_df.sort_index()
comp_scores['final_test_scores'] = final_test_scores_df['final_test_scores']
comp_scores = comp_scores.sort_values(by='final_test_scores')
sns.set(style="darkgrid")


plt.figure(figsize=(15, 5))
df_reset = comp_scores.reset_index()
print(df_reset)
df_melted = df_reset.melt(id_vars='index', var_name='Metric', value_name='Value')


sns.barplot(x='index', y='Value', hue='Metric', data=df_melted, palette="pastel")
plt.xlabel("Model")
plt.ylabel("MAE")
plt.title("MAE vs Models")
plt.legend(title="Metrics")
#plt.show()

scores

all_models = ['Linear Reg',  'Elastic Net Reg',  'Random Forest Reg', 'AdaBoost Reg']
overall_scores = pd.DataFrame(scores,index=all_models)
overall_scores

'''Data frame of our optimized base model predictions.'''
df_test_final = X_test
base_model_prediction = pd.DataFrame({'LR':linear.predict(df_test_final), 'AB':ab_opt.predict(df_test_final),'Elastic Net':elastic_net_opt.predict(df_test_final), 'RF':rf_opt.predict(df_test_final)})

# display(base_model_prediction.head())

"""Let's visualize the correlations among the predictions of base models."""
# fig, ax = plt.subplots(figsize = (22, 8))
sns.heatmap(base_model_prediction.corr(), annot = True)
plt.title('Prediction Correlation among the Base Models', fontsize = 22)
#plt.show()

"""**Ensemble methods**"""

voting = VotingRegressor([('LR',linear), ('RF',rf_opt), ('AB',ab_opt), ('E_net',elastic_net_opt)])
voting.fit(X_train, y_train)
y_pred_train = voting.predict(X_train)
y_pred_test = voting.predict(X_test)
report_results(y_pred_train,y_pred_test)
all_models.append('Voting Reg')
print('mean_absolute_error',mean_absolute_error(y_test, y_pred_test))

overall_scores = pd.DataFrame(scores,index=all_models)

bagging = BaggingRegressor(random_state=23,
                           n_jobs=-1)

params = {'estimator': [linear,rf_opt,ab_opt,elastic_net_opt],
          'n_estimators':[10]}
gcv = GridSearchCV(bagging, param_grid=params,n_jobs=-1,
                   cv=2, scoring='neg_mean_absolute_error')
gcv.fit(X_train, y_train)
print(gcv.best_params_)
print(gcv.best_score_)
bagging = BaggingRegressor(**gcv.best_params_)
bagging.fit(X_train, y_train)

y_pred_train = bagging.predict(X_train)
y_pred_test = bagging.predict(X_test)
report_results(y_pred_train,y_pred_test)
all_models.append('Bagging Reg')
overall_scores = pd.DataFrame(scores,index=all_models)

models = [('LR',linear), ('RF',rf_opt), ('AB',ab_opt), ('E_net',elastic_net_opt)]
stack = StackingRegressor(estimators=models,final_estimator=rf_opt,
                           passthrough=True)

####### Grid Search
print(stack.get_params())
kfold = KFold(n_splits=5, shuffle=True, random_state=23)
params = { 'final_estimator':[rf_opt,ab_opt]}
rgcv = RandomizedSearchCV(stack, n_iter=20, n_jobs=-1,
                          param_distributions=params,
                   cv=kfold, scoring='neg_mean_absolute_error')
rgcv.fit(X_train, y_train)
print(rgcv.best_params_)
print(rgcv.best_score_)

stacking = rgcv.best_estimator_
stacking.fit(X_train, y_train)
y_pred_train = stacking.predict(X_train)
y_pred_test = stacking.predict(X_test)
report_results(y_pred_train,y_pred_test)
all_models.append('Stacking Reg')
overall_scores = pd.DataFrame(scores,index=all_models)
overall_scores

"""
**MLP Regressor**"""

"""define the shallow multi-layer perceptron model"""
mlp = MLPRegressor(hidden_layer_sizes=(10,), max_iter=1000, n_iter_no_change=100, activation='relu',
                   solver='adam', verbose=False, warm_start=False)
print(cross_validate(mlp))
print(predict_with_optimized_models(mlp))
all_models.append('MLP')
print(scores)
overall_scores = pd.DataFrame(scores,index=all_models)
overall_scores

"""**Deep Neural Network**"""

tf.random.set_seed(2023)
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(15, activation='relu',input_shape=(X_trn_scl.shape[1],)),
    tf.keras.layers.Dense(15, activation='relu'),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(5, activation='relu'),
    tf.keras.layers.Dense(3, activation='relu'),
    tf.keras.layers.Dense(1, activation='linear')
])


batch_size = 16
epochs = 1000
STEPS_PER_EPOCH = X_trn_scl.shape[0]/batch_size
es = keras.callbacks.EarlyStopping(monitor='loss', patience=50, verbose=0)

lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
  0.001,
  decay_steps = STEPS_PER_EPOCH * 100,
  decay_rate=0.98,
  staircase=False
)

opt = keras.optimizers.Adam(lr_schedule)

model.compile(loss='mae', optimizer=opt, metrics=['mae'])
model.summary()

history  = model.fit(X_trn_scl,y_trn_scl, epochs=epochs, batch_size=batch_size, callbacks=[es], verbose=1)

from sklearn.metrics import mean_absolute_error
y_pred_ = model.predict(X_tst_scl)
y_pred = scaler_y.inverse_transform(y_pred_)
MAE = mean_absolute_error(y_true=y_test,y_pred=y_pred)
print(f"MAE: {MAE}")

y_pred_train_ = model.predict(X_trn_scl)
y_pred_train = scaler_y.inverse_transform(y_pred_train_)
y_pred_test_ = model.predict(X_tst_scl)
y_pred_test = scaler_y.inverse_transform(y_pred_test_)
report_results(y_pred_train,y_pred_test)

"""**Model Comparisons**"""

all_models.append('DeepMLP')
print(scores)
overall_scores = pd.DataFrame(scores,index=all_models)
overall_scores

df['trip_distance(km)'].std()

comp_scores = overall_scores[['MAE_train','MAE_test']]
comp_scores = comp_scores.sort_values(by='MAE_test')
sns.set(style="darkgrid")
plt.figure(figsize=(20, 5))

df_reset = comp_scores.reset_index()
print(df_reset)
df_melted = df_reset.melt(id_vars='index', var_name='Metric', value_name='Value')

sns.barplot(x='index', y='Value', hue='Metric', data=df_melted, palette="pastel")
plt.xlabel("Model")
plt.ylabel("MAE")
plt.title("MAE vs Models")

plt.legend(title="Metrics")

#plt.show()

"""**Learning Curves**"""

import warnings
warnings.filterwarnings('ignore')

# from yellowbrick.datasets import load_energy
# from yellowbrick.model_selection import LearningCurve

# # Load a regression dataset
# y = df['trip_distance(km)']
# X = df.drop('trip_distance(km)',axis=1)


# models = [linear, elastic_net_opt, rf_opt,ab_opt,voting,bagging,stacking,mlp]
# #Instantiate the regression model and visualizer
# model = models[0]
# for i in models:

#     visualizer = LearningCurve(i, scoring='neg_mean_absolute_error')

#     visualizer.fit(X, y)        # Fit the data to the visualizer
#     visualizer.show()           # Finalize and render the figure

# """**Feature Importances**"""

# from sklearn.ensemble import RandomForestClassifier

# from yellowbrick.datasets import load_occupancy
# from yellowbrick.model_selection import FeatureImportances

# # Load the classification data set

# models = [linear, elastic_net_opt, rf_opt,ab_opt]

# for model in models:
#     viz = FeatureImportances(model)
#     viz.fit(X, y)
#     viz.show()

# all_models

models = [linear, elastic_net_opt,rf_opt,ab_opt,voting,bagging,stacking,mlp,model]

best_score = overall_scores['MAE_test'].min()
best_model_name = overall_scores['MAE_test'].idxmin()
best_model = models[all_models.index(best_model_name)]
print(best_score)
print(best_model_name)
print(best_model)

import pickle

if best_model_name == 'DeepMLP':
    best_model.save(out_path_parent+"model_r.keras")
    print(1)
else:
    with open(out_path_parent+'model_r.pkl', 'wb') as files:
        pickle.dump(best_model, files)


with open(out_path_parent+'scaler_X_r.pkl', 'wb') as files:
        pickle.dump(scaler_X, files)
with open(out_path_parent+'scaler_y_r.pkl', 'wb') as files:
        pickle.dump(scaler_y, files)


import csv

model_details = {
    'mae': best_score,
    'params': str(best_model),
    'name': best_model_name
}

csv_file = out_path_parent+'model_details.csv'

# Open the CSV file in write mode
with open(csv_file, 'w', newline='') as csvfile:
    field_names = model_details.keys()

    # Create a CSV writer and write the header
    writer = csv.DictWriter(csvfile, fieldnames=field_names)
    writer.writeheader()

    # Write the model details as a row
    writer.writerow(model_details)